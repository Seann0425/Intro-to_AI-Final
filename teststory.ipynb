{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install bart_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftfcY588LOXE",
        "outputId": "b24de6ab-c9a8-4ff8-81c2-64ab4b45a9bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement bart_score (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for bart_score\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import traceback\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "from typing import List\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class BARTScorer:\n",
        "    def __init__(self, device='cuda:0', max_length=1024, checkpoint='facebook/bart-large-cnn'):\n",
        "        # Set up model\n",
        "        self.device = device\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = BartTokenizer.from_pretrained(checkpoint)\n",
        "        self.model = BartForConditionalGeneration.from_pretrained(checkpoint)\n",
        "        self.model.eval()\n",
        "        self.model.to(device)\n",
        "\n",
        "        # Set up loss\n",
        "        self.loss_fct = nn.NLLLoss(reduction='none', ignore_index=self.model.config.pad_token_id)\n",
        "        self.lsm = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def load(self, path=None):\n",
        "        \"\"\" Load model from paraphrase finetuning \"\"\"\n",
        "        if path is None:\n",
        "            path = 'models/bart.pth'\n",
        "        self.model.load_state_dict(torch.load(path, map_location=self.device))\n",
        "\n",
        "    def score(self, srcs, tgts, batch_size=4):\n",
        "        \"\"\" Score a batch of examples \"\"\"\n",
        "        score_list = []\n",
        "        for i in range(0, len(srcs), batch_size):\n",
        "            src_list = srcs[i: i + batch_size]\n",
        "            tgt_list = tgts[i: i + batch_size]\n",
        "            try:\n",
        "                with torch.no_grad():\n",
        "                    encoded_src = self.tokenizer(\n",
        "                        src_list,\n",
        "                        max_length=self.max_length,\n",
        "                        truncation=True,\n",
        "                        padding=True,\n",
        "                        return_tensors='pt'\n",
        "                    )\n",
        "                    encoded_tgt = self.tokenizer(\n",
        "                        tgt_list,\n",
        "                        max_length=self.max_length,\n",
        "                        truncation=True,\n",
        "                        padding=True,\n",
        "                        return_tensors='pt'\n",
        "                    )\n",
        "                    src_tokens = encoded_src['input_ids'].to(self.device)\n",
        "                    src_mask = encoded_src['attention_mask'].to(self.device)\n",
        "\n",
        "                    tgt_tokens = encoded_tgt['input_ids'].to(self.device)\n",
        "                    tgt_mask = encoded_tgt['attention_mask']\n",
        "                    tgt_len = tgt_mask.sum(dim=1).to(self.device)\n",
        "\n",
        "                    output = self.model(\n",
        "                        input_ids=src_tokens,\n",
        "                        attention_mask=src_mask,\n",
        "                        labels=tgt_tokens\n",
        "                    )\n",
        "                    logits = output.logits.view(-1, self.model.config.vocab_size)\n",
        "                    loss = self.loss_fct(self.lsm(logits), tgt_tokens.view(-1))\n",
        "                    loss = loss.view(tgt_tokens.shape[0], -1)\n",
        "                    loss = loss.sum(dim=1) / tgt_len\n",
        "                    curr_score_list = [-x.item() for x in loss]\n",
        "                    score_list += curr_score_list\n",
        "\n",
        "            except RuntimeError:\n",
        "                traceback.print_exc()\n",
        "                print(f'source: {src_list}')\n",
        "                print(f'target: {tgt_list}')\n",
        "                exit(0)\n",
        "        return score_list\n",
        "\n",
        "    def multi_ref_score(self, srcs, tgts: List[List[str]], agg=\"mean\", batch_size=4):\n",
        "        # Assert we have the same number of references\n",
        "        ref_nums = [len(x) for x in tgts]\n",
        "        if len(set(ref_nums)) > 1:\n",
        "            raise Exception(\"You have different number of references per test sample.\")\n",
        "\n",
        "        ref_num = len(tgts[0])\n",
        "        score_matrix = []\n",
        "        for i in range(ref_num):\n",
        "            curr_tgts = [x[i] for x in tgts]\n",
        "            scores = self.score(srcs, curr_tgts, batch_size)\n",
        "            score_matrix.append(scores)\n",
        "        if agg == \"mean\":\n",
        "            score_list = np.mean(score_matrix, axis=0)\n",
        "        elif agg == \"max\":\n",
        "            score_list = np.max(score_matrix, axis=0)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        return list(score_list)\n",
        "\n",
        "    def test(self, batch_size=3):\n",
        "        \"\"\" Test \"\"\"\n",
        "        src_list = [\n",
        "            'This is a very good idea. Although simple, but very insightful.',\n",
        "            'Can I take a look?',\n",
        "            'Do not trust him, he is a liar.'\n",
        "        ]\n",
        "\n",
        "        tgt_list = [\n",
        "            \"That's stupid.\",\n",
        "            \"What's the problem?\",\n",
        "            'He is trustworthy.'\n",
        "        ]\n",
        "\n",
        "        print(self.score(src_list, tgt_list, batch_size))"
      ],
      "metadata": {
        "id": "tljcuS8_MEGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fqSlbMyJSZt",
        "outputId": "e12e7c5e-3e98-4b88-c484-603538d92bb1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-3.5862600803375244]"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ],
      "source": [
        "bart_scorer = BARTScorer(device='cuda:0', checkpoint='facebook/bart-large-cnn')\n",
        "bart_scorer.score(['Craft a heartbreaking tale centered around siblings Eliza and Ethan in the town of Willow Creek. When Eliza becomes gravely ill, Ethan strikes a bargain with a shadowy figure to restore her health, oblivious to the dire consequences it will entail.'],\n",
        "\n",
        "[\"\"\"In the serene town of Willow Creek, where the gentle babbling of brooks intertwined with the rustling leaves of ancient oaks, lived siblings Eliza and Ethan. The twins shared an unbreakable bond, their lives entwined like the ivy that clung to their family home. Eliza's laughter was the sunlight in Ethan's world, and his strength was her shield.\n",
        "\n",
        "Their peaceful life took a dark turn one autumn when Eliza fell gravely ill. The once vibrant girl, known for her infectious smile and boundless energy, was now confined to her bed, her health deteriorating rapidly. No doctor in Willow Creek or the neighboring towns could diagnose her mysterious ailment, and hope began to fade as winter's chill set in.\n",
        "\n",
        "Desperate and unwilling to watch his sister slip away, Ethan ventured into the woods one moonless night. He had heard whispers of a shadowy figure who could perform miracles, a being shrouded in mystery and fear. Ethan's heart pounded with both dread and determination as he followed the cryptic directions given by an old man in the village square.\n",
        "\n",
        "Deep in the forest, under a canopy of twisted branches, Ethan found the shadowy figure. Cloaked in darkness, the figure's eyes glowed with an eerie light. \"What brings you here, young one?\" the figure's voice was a soft hiss, like the wind through dead leaves.\n",
        "\n",
        "\"My sister, Eliza, is dying,\" Ethan replied, his voice trembling. \"I'll do anything to save her.\"\n",
        "\n",
        "The figure smiled, a chilling sight. \"Anything, you say? Very well. I can restore your sister's health, but such a gift requires a sacrifice.\"\n",
        "\n",
        "Without hesitation, Ethan nodded. \"Take whatever you need. Just save her.\"\n",
        "\n",
        "The figure extended a hand, and as Ethan grasped it, a cold pain shot through his body. \"The bargain is struck,\" the figure whispered, vanishing into the night.\n",
        "\n",
        "The next morning, Eliza awoke with a newfound vigor. Her cheeks were flushed with color, and her laughter filled the house once more. The town marveled at her miraculous recovery, and Ethan's heart soared with joy. But as days turned into weeks, Ethan began to feel the effects of his bargain.\n",
        "\n",
        "A shadow seemed to hang over him, draining his energy and clouding his mind. He grew weaker with each passing day, his once strong body now frail and exhausted. The cost of the miracle became painfully clear—Eliza's life had been restored at the expense of his own.\n",
        "\n",
        "Ethan tried to hide his decline, but Eliza noticed the change in her brother. One evening, as they sat by the fireplace, she confronted him. \"Ethan, what's happening to you? You look so tired and sick.\"\n",
        "\n",
        "Tears welled in Ethan's eyes. \"I made a deal to save you, Eliza. A deal with a shadowy figure in the woods. This is the price I have to pay.\"\n",
        "\n",
        "Horrified, Eliza grasped his hand. \"No, Ethan. We have to find a way to reverse it. I can't lose you because of me.\"\n",
        "\n",
        "But Ethan knew it was too late. The bargain was unbreakable, and his time was running out. He spent his remaining days cherishing every moment with Eliza, her laughter now tinged with sorrow.\n",
        "\n",
        "On a cold winter's night, as snowflakes danced outside their window, Ethan took his final breath. Eliza held him close, her heart shattered by the loss of her beloved brother. In his last moments, Ethan whispered, \"Live for both of us, Eliza.\"\n",
        "\n",
        "In the aftermath of Ethan's death, Willow Creek mourned the loss of a brave young man. Eliza, though heartbroken, vowed to honor Ethan's sacrifice by living a life full of love and kindness. She carried his memory with her, a beacon of hope and resilience.\n",
        "\n",
        "And in the quiet moments, when the wind whispered through the trees, Eliza could almost hear Ethan's voice, a reminder of the unbreakable bond they shared—a bond that even death could not sever.\"\"\"], batch_size=4) # generation scores from"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LR-n 算用詞多樣性\n",
        "def compute_lr_n(text, n):\n",
        "    from collections import Counter\n",
        "    from nltk import ngrams\n",
        "    import nltk\n",
        "\n",
        "    # Ensure that the necessary NLTK data is available\n",
        "    nltk.download('punkt')\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Extract n-grams\n",
        "    n_grams = list(ngrams(words, n))\n",
        "\n",
        "    # Count occurrences of each n-gram\n",
        "    n_gram_counts = Counter(n_grams)\n",
        "\n",
        "    # Calculate the number of repeated n-grams\n",
        "    repeated_n_grams = sum(1 for count in n_gram_counts.values() if count > 1)\n",
        "\n",
        "    # Calculate total number of n-grams\n",
        "    total_n_grams = len(n_grams)\n",
        "\n",
        "    # Compute LR-n as the ratio of repeated n-grams to total n-grams\n",
        "    if total_n_grams == 0:\n",
        "        return 0\n",
        "    lr_n = repeated_n_grams / total_n_grams\n",
        "\n",
        "    return lr_n\n",
        "\n",
        "# Example usage\n",
        "text = \"\"\"\n",
        "Cat cat cat cat cat cat cat cat\"\"\"\n",
        "\n",
        "n = 2  # Change this to any value of n for LR-n\n",
        "lr_n = compute_lr_n(text, n) * 2\n",
        "print(f'LR-{n}: {lr_n}')"
      ],
      "metadata": {
        "id": "CePKhJVpBaIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# D3 也是算用詞多樣性\n",
        "def compute_d3(text):\n",
        "    from collections import Counter\n",
        "    from nltk import ngrams\n",
        "    import nltk\n",
        "\n",
        "    # Ensure that the necessary NLTK data is available\n",
        "    nltk.download('punkt')\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Extract 3-grams\n",
        "    three_grams = list(ngrams(words, 3))\n",
        "\n",
        "    # Count occurrences of each 3-gram\n",
        "    three_gram_counts = Counter(three_grams)\n",
        "\n",
        "    # Calculate the number of unique 3-grams\n",
        "    unique_three_grams = sum(1 for count in three_gram_counts.values() if count == 1)\n",
        "\n",
        "    # Calculate total number of 3-grams\n",
        "    total_three_grams = len(three_grams)\n",
        "\n",
        "    # Compute D-3 as the ratio of unique 3-grams to total 3-grams\n",
        "    if total_three_grams == 0:\n",
        "        return 0\n",
        "    d3 = unique_three_grams / total_three_grams\n",
        "\n",
        "    return d3\n",
        "\n",
        "# Example usage\n",
        "text = \"\"\"cat mat hat pat sat\"\"\"\n",
        "\n",
        "d3 = compute_d3(text)\n",
        "print(f'D-3: {d3}')"
      ],
      "metadata": {
        "id": "JO10rrvFBv7V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}